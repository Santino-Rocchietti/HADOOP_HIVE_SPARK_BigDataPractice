Para hacer el primer ejercicio:

Crear el fork del repositorio y usar git clone para clonarlo
Usar cd para moverse a la carpeta creada

Para el primer Ejercicio:
Ejecutar el docker compose 1 (sudo docker-compose -f docker-compose-v1.yml up -d)
Dar permisos a Ejercicio1.sh: (chmod u+x Ejercicio1.sh) y ejecutarlo (./Ejercicio1.sh)

Para el segundo Ejercicio:
Ejecutar el docker compose 2 (sudo docker-compose -f docker-compose-v2.yml up -d)
Dar permisos a Ejercicio2.sh: (chmod u+x Ejercicio2.sh) y ejecutarlo (./Ejercicio2.sh)
Modifique el Paso02.hql agregando un EXIT al final.

Para el tercer Ejercicio:

1. Copiar los datos en data2 del HDFS

2. Modificar el Paso03.hql (al final) para la consulta de agregación x12

2. Dar permisos a Ejercicio3.sh: 
``` chmod u+x Ejercicio3.sh ```
3. Ejecutar: 
``` ./Ejercicio3.sh ``


El ejercicio 4 esta explicito en el archivo cargado.

El ejercicio 5 muestra distintas cosas
HBase
Creación y manejo de datos:

Crea la tabla personal con la columna personal_data.
Inserta datos en la tabla y consulta los datos almacenados.
Importación desde CSV:

Sube un archivo CSV a HDFS.
Utiliza el comando ImportTsv para cargar los datos desde el CSV a la tabla personal en HBase.
Operaciones adicionales:

Crea y manipula otra tabla llamada album.
MongoDB
Importación de datos:

Copia archivos CSV y JSON al contenedor MongoDB.
Usa mongoimport para cargar datos desde estos archivos a las colecciones iris_csv e iris_json.
Consulta y exportación de datos:

Accede a MongoDB, consulta los datos en las colecciones y exporta los datos a archivos CSV y JSON.
Configuración para Hive:

Descarga e importa JARs necesarios para integrar MongoDB con Hive, incluyendo drivers y conectores específicos.
Neo4J
Ejemplo de búsqueda del camino más corto:

Crea nodos y relaciones para representar ubicaciones y caminos.
Usa el algoritmo de Dijkstra para encontrar el camino más corto entre dos ubicaciones.
Realiza la búsqueda del árbol de expansión mínima (MST) para análisis de logística.
Carga de datos:

Copia archivos CSV a la carpeta de importación de Neo4J y realiza las operaciones necesarias para cargar y consultar estos datos.
Zeppelin
Configuración de HDFS en Zeppelin:

Utiliza WebHDFS para interactuar con HDFS desde Zeppelin.
Configura el intérprete de HDFS en Zeppelin para facilitar la interacción con HDFS.
Configuración de Neo4J en Zeppelin:

Configura el intérprete de Neo4J en Zeppelin para conectarte a Neo4J y realizar consultas desde notebooks en Zeppelin.


Pasando al ejercicio 6 una vez usado todos los codigos primero cargue el archivo Parquet

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión de Spark
spark = SparkSession.builder \
    .appName("OutlierDetection") \
    .getOrCreate()

# Cargar los datos desde el archivo Parquet
venta_df = spark.read.parquet("hdfs://namenode:9000/data/venta")

Examine los datos:

# Mostrar las primeras filas del DataFrame
venta_df.show()

# Ver el esquema del DataFrame
venta_df.printSchema()

Para el filtrado de outliers solo seleccione las columas que queria utilizaer a traves de df.select, lo filtre con df_filtered.approxQuantile y guarde el DataFrame modificado


Para el ejercicio 7 use el Paso06_IncrementalVentas.py, cree un script de shell y configure el crontab
